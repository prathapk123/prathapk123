Poster:
15784595.pdf (stanford.edu)
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/posters/15784595.pdf
Paper:
15784595.pdf (stanford.edu)
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15784595.pdf

Transformers and Pointer-Generator Networks for Abstractive Summarization
kACL 2019
BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization (aclweb.org)
https://www.aclweb.org/anthology/P19-1207.pdf

Evaluating the Factual Consistency of Abstractive Text Summarization
2010.08712.pdf (arxiv.org)
https://arxiv.org/pdf/2010.08712.pdf

GitHub - yuhui-zh15/FactCCX

https://www.diva-portal.org/smash/get/diva2:1368180/FULLTEXT01.pdf
All improvements in the area:



https://github.com/Developer-Zer0/Get-To-The-Point-Summarization-with-Pointer-Generator-Networks
https://nlp.stanford.edu/pubs/see2017get.pdf

https://github.com/wprojectsn/codes
https://paperswithcode.com/paper/concept-pointer-network-for-abstractive
Concept Pointer Network for Abstractive Summarization

https://paperswithcode.com/paper/neural-abstractive-text-summarization-with
https://github.com/tshi04/NATS

very easy: Easy Factual Error Correction:
https://github.com/mcao610/Factual-Error-Correction
https://arxiv.org/abs/2010.08712

ICLR 2018
https://paperswithcode.com/paper/a-deep-reinforced-model-for-abstractive
A Deep Reinforced Model for Abstractive Summarization

https://github.com/tshi04/NATS
https://paperswithcode.com/paper/neural-abstractive-text-summarization-with
Neural Abstractive Text Summarization with Sequence-to-Sequence Models
dec,2018 

NeurIPS: Big Bird: Transformers for Longer Sequences
https://paperswithcode.com/paper/big-bird-transformers-for-longer-sequences#code


https://paperswithcode.com/paper/neural-abstractive-text-summarization-with
https://github.com/tshi04/NATS

ok ok more files but easy 
Sample Efficient Text Summarization Using a Single Pre-Trained Transformer
https://github.com/marumalo/pytorch-translm

very very easy check if this can be natively used-multiple techniques
https://github.com/john-mai-2605/CS492-Auto-Summarization

# A Abstractive Summarization Implementation with Transformer and Pointer-generator
when I wanted to get summary by neural network, I tried many ways to generate abstract summary, but the result was not good.
when I heared 2018 byte cup, I found some information about it, and the champion's solution attracted me, but I found some websites,
like github gitlab, I didn't find the official code, so I decided to implement it.

## Model Structure
### Based
My model is based on [Attention Is All You Need](https://arxiv.org/abs/1706.03762) and [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)
### Change
* The pointer-generator model has two mechanisms, which are **copy mechanism** and **coverage mechanism**, I found some materials, 
they show the Coverage mechanism doesn't suit short summary, so I didn't use this mechanism, just use the first one.
* Pointer generator model has a inadequacy, which can let the loss got nan, I tried some times and wanted to fix it,
but the result was I can't, I think the reason was when calculate final logists, it will 
 extend vocab length to oov and vocab length, it will get more zeroes. so I delete the mechanism of extend final logists, just use their mechanism of 
deocode from article and vocab. there is more [detail](https://github.com/abisee/pointer-generator/issues/4) about it, 
in this model, I just use word than vocab, this idea is from bert.
### Structure

https://arxiv.org/pdf/1704.04368.pdf
https://arxiv.org/pdf/2010.08712.pdf
https://arxiv.org/pdf/1812.02303.pdf
